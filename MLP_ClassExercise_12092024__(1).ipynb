{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rislam21/Data-engineering/blob/main/MLP_ClassExercise_12092024__(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QleWkNk1Qr8c"
      },
      "source": [
        "\n",
        "**Exercise 1: Implementing a Single Neuron**\n",
        "\n",
        "**Objective:** Understand the basic structure of a neuron.\n",
        "\n",
        "\n",
        "**Task:** Write a Python function that implements a single neuron. The neuron should take an input vector **X**\n",
        "and a weight vector **W**, apply a linear combination, and then apply a sigmoid activation function.\n",
        "\n",
        "$z = \\sigma\\left(\\sum_{i=1}^{n} w_i x_i + b\\right)$\n",
        "\n",
        "Hint: Here is the sigmoid ($\\sigma$)function formula:\n",
        "\n",
        "$\\sigma(x) = \\frac{1}{1 + e^{-x}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cF5N6S3hRZRy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e58aecbf-0cec-46ad-cfd7-bb291b6dc1a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: 0.6509005438802886\n",
            "Expected Output: 0.6509005438802886\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid function.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : array_like\n",
        "        Data point values for x-axis.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    z : float\n",
        "      a value between 0 and 1.\n",
        "    \"\"\"\n",
        "    ####### Your Code Here #########\n",
        "    z = 1 / (1 + np.exp(-x))\n",
        "    ################################\n",
        "    return z\n",
        "\n",
        "def single_neuron(x, w, b):\n",
        "    \"\"\"\n",
        "    Implement the forward pass. Calculate the weighted sum (z = w*x + b)\n",
        "    Hint: Use np.dot for the weighted sum, then apply sigmoid\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : array_like\n",
        "        Data point values (input values).\n",
        "    w : array_like\n",
        "        Data point values (random weights).\n",
        "    b : float\n",
        "        Data point values (bias).\n",
        "    Returns\n",
        "    -------\n",
        "    z : float\n",
        "      a value between 0 and 1.\n",
        "    \"\"\"\n",
        "    ####### Your Code Here #########\n",
        "    z = np.dot(w, x) + b\n",
        "    ################################\n",
        "    return sigmoid(z)\n",
        "\n",
        "# Test the function with some inputs\n",
        "x = np.array([0.0, 0.5, -0.2, 0.1, 0.4, 0.3, -0.88, 0.4, 0.15, 0.69])\n",
        "w = np.array([0.4, 0.6, -0.8, 0.2, 0.3, 0.4, 0.6, -0.5, -0.6, 0.9])\n",
        "b = 0.1\n",
        "output = single_neuron(x, w, b)\n",
        "print(\"Output:\", output)\n",
        "print(\"Expected Output: 0.6509005438802886\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9TslqjzS76b"
      },
      "source": [
        "**Exercise 2: Implementing a Simple Layer**\n",
        "\n",
        "**Objective:** Learn to stack neurons into a single layer.\n",
        "\n",
        "**Task:** Extend the single neuron into a layer of neurons. Implement a function that takes an input vector, a matrix of weights, and a vector of biases, and returns the output of the layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rW9IPFzUTMMR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "019c555f-7f0b-41ba-b50f-c4645d0a239d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer Output: [0.52497919 0.50749944]\n",
            "Expected Layer Output: [0.52497919 0.50749944]\n"
          ]
        }
      ],
      "source": [
        "def layer(x, W, b):\n",
        "\n",
        "    \"\"\"\n",
        "    Extend the single neuron into a layer of neurons.\n",
        "    Implement a function that takes an input vector, a matrix of weights, and a vector of biases, and returns the output of the layer.\n",
        "    ----------\n",
        "    x : array_like\n",
        "        Data point values (input values).\n",
        "    W :  array_like\n",
        "        multidimensional array of data point values (weights).\n",
        "    b : array_like\n",
        "        Data point values (bias).\n",
        "    Returns\n",
        "    -------\n",
        "    z : array_like\n",
        "       output values between 0 and 1.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    ####### Your Code Here #########\n",
        "\n",
        "    z = np.dot(W, x) + b\n",
        "\n",
        "    ################################\n",
        "\n",
        "\n",
        "    return sigmoid(z)\n",
        "\n",
        "# Test the function with some inputs\n",
        "x = np.array([0.5, -0.2, 0.1])\n",
        "W = np.array([[0.4, 0.6, -0.8],\n",
        "              [0.2, -0.5, 0.3]])\n",
        "b = np.array([0.1, -0.2])\n",
        "output = layer(x, W, b)\n",
        "print(\"Layer Output:\", output)\n",
        "print(\"Expected Layer Output: [0.52497919 0.50749944]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jY_B2yqDxoq"
      },
      "source": [
        "**Exercise 2: Implementing a Simple Layer**\n",
        "\n",
        "**Objective:** Learn to stack neurons into a single layer.\n",
        "\n",
        "**Task:** Extend the single neuron into a layer of neurons. Implement a function that takes an input vector, a matrix of weights, and a vector of biases, and returns the output of the layer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "my34jl-AVSKs"
      },
      "source": [
        "**Exercise 3: Building a Two-Layer MLP**\n",
        "\n",
        "**Objective:** Understand how to combine multiple layers into a network.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9h0D_cnmVOJd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cdb7f1c-aae7-4b90-df24-59ea0f45ba20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP Output: [0.56579959 0.5265988 ]\n",
            "Expected MLP Output: [0.56579959 0.5265988]\n"
          ]
        }
      ],
      "source": [
        "def two_layer_mlp(x, W1, b1, W2, b2):\n",
        "  \"\"\"\n",
        "  Implement a simple two-layer MLP by stacking two layers from the previous exercise.\n",
        "  The first layer should have 3 neurons, and the second layer should have 2 neurons.\n",
        "  x : array_like\n",
        "      Data point values (input values).\n",
        "  W1 :  array_like\n",
        "      multidimensional array of data point values (weights).\n",
        "  b1 : array_like\n",
        "      Data point values (bias)\n",
        "\n",
        "  W2 :  array_like\n",
        "      multidimensional array of data point values (weights).\n",
        "  b2 : array_like\n",
        "      Data point values (bias)\n",
        "  Returns\n",
        "  -------\n",
        "  output : array_like\n",
        "      output values between 0 and 1.\n",
        "  \"\"\"\n",
        "  ####### Your Code Here #########\n",
        "  # First layer  Hint: use the layer function (neuron) you built previously.\n",
        "  layer1_output = layer(x, W1, b1)\n",
        "  # Second layer\n",
        "  output = layer(layer1_output, W2, b2)\n",
        "\n",
        "  ################################\n",
        "\n",
        "  return output\n",
        "\n",
        "# Test the function with some inputs\n",
        "x = np.array([0.5, -0.2, 0.1])\n",
        "W1 = np.array([[0.4, 0.6, -0.8],\n",
        "               [0.2, -0.5, 0.3],\n",
        "               [-0.3, 0.7, 0.9]])\n",
        "b1 = np.array([0.1, -0.2, 0.3])\n",
        "\n",
        "W2 = np.array([[0.3, -0.7, 0.5],\n",
        "               [-0.6, 0.2, 0.8]])\n",
        "b2 = np.array([0.2, -0.1])\n",
        "\n",
        "output = two_layer_mlp(x, W1, b1, W2, b2)\n",
        "print(\"MLP Output:\", output)\n",
        "print(\"Expected MLP Output: [0.56579959 0.5265988]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfthVtSCck5s"
      },
      "source": [
        "**Exercise 4: Example of MLP for MNIST Digit Recognition in TensorFlow**\n",
        "\n",
        "**Objective:** Python code for classifying the MNIST dataset using a Multilayer Perceptron (MLP) with Keras (TensorFlow backend). This MLP includes one hidden layer with ReLU activation and is trained using the Adam optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eMyYoc9-kcFo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40dc50e4-a956-4045-d48e-cc2ac81a39e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.8563 - loss: 0.4658\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.9665 - loss: 0.1098\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.9771 - loss: 0.0728\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9825 - loss: 0.0556\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9859 - loss: 0.0435\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7fdf9b217940>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.reshape((x_train.shape[0], 28 * 28)) / 255.0\n",
        "y_train = to_categorical(y_train)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(128, activation='relu', input_shape=(28 * 28,)))\n",
        "model.add(Dense(64, activation='relu' ))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qeLK0HOlFxcR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        },
        "outputId": "bae7d843-4ff6-45e7-9f51-54c7be4bd1d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.8329 - loss: 0.6037 - val_accuracy: 0.9537 - val_loss: 0.1552\n",
            "Epoch 2/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9570 - loss: 0.1448 - val_accuracy: 0.9617 - val_loss: 0.1213\n",
            "Epoch 3/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9731 - loss: 0.0915 - val_accuracy: 0.9719 - val_loss: 0.0914\n",
            "Epoch 4/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9787 - loss: 0.0697 - val_accuracy: 0.9715 - val_loss: 0.0882\n",
            "Epoch 5/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9832 - loss: 0.0560 - val_accuracy: 0.9738 - val_loss: 0.0844\n",
            "Epoch 6/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9864 - loss: 0.0458 - val_accuracy: 0.9789 - val_loss: 0.0720\n",
            "Epoch 7/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9899 - loss: 0.0334 - val_accuracy: 0.9757 - val_loss: 0.0799\n",
            "Epoch 8/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9911 - loss: 0.0292 - val_accuracy: 0.9785 - val_loss: 0.0734\n",
            "Epoch 9/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9925 - loss: 0.0246 - val_accuracy: 0.9764 - val_loss: 0.0826\n",
            "Epoch 10/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9941 - loss: 0.0197 - val_accuracy: 0.9771 - val_loss: 0.0812\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9725 - loss: 0.0936\n",
            "Test Accuracy: 0.9771000146865845\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGzCAYAAABpdMNsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAjIElEQVR4nO3de3BU9f3/8dcGyHJLFgPkRiAGBGm5WREixQJK5FoKalux/gGOVcRgReqleAEVbVraWlpLsa0d0gteSqfAlFo6gCSUFmhFGAarCJlYQJKgVHYhQEDy+f3Bj/2yJiGcZTfvTXg+Zj4z7DnnvefN4ZBXzu7Zz/qcc04AADSxJOsGAACXJwIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAgiIwpVXXqnp06eHH5eUlMjn86mkpMSsp8/6bI9AoiGA0OwUFxfL5/OFR9u2bdWnTx/NmjVLVVVV1u158sYbb+jpp5+2bqNetbW1WrhwofLy8tS2bVsNHDhQr776qnVbaEFaWzcAROvZZ59VXl6eTp48qU2bNmnJkiV64403tGvXLrVv375JexkxYoROnDih5ORkT3VvvPGGFi9enJAh9MQTT+h73/ue7rnnHg0ZMkSrVq3SN77xDfl8Pk2dOtW6PbQABBCarfHjx+u6666TJH3zm99U586d9cILL2jVqlW644476q2prq5Whw4dYt5LUlKS2rZtG/PntfLhhx/qRz/6kQoLC/Wzn/1M0tljPHLkSD3yyCP62te+platWhl3ieaOl+DQYtx0002SpPLycknS9OnT1bFjR5WVlWnChAlKSUnRnXfeKensy0uLFi1Sv3791LZtW2VkZGjGjBn65JNPIp7TOafnnntOOTk5at++vW688Ua98847dfbd0HtAW7du1YQJE3TFFVeoQ4cOGjhwoH7yk5+E+1u8eLEkRbykeE6se5SksrIylZWVNXosV61apdOnT+v+++8PL/P5fJo5c6YOHDigzZs3N/ocQGO4AkKLce4Ha+fOncPLPv30U40dO1Y33HCDfvjDH4ZfmpsxY4aKi4t111136Vvf+pbKy8v1s5/9TNu3b9c//vEPtWnTRpI0b948Pffcc5owYYImTJigt99+W2PGjNGpU6ca7Wft2rX68pe/rKysLD344IPKzMzUu+++q9WrV+vBBx/UjBkzdPDgQa1du1a/+93v6tTHo8fRo0dLkj744IML9r59+3Z16NBBn/vc5yKWDx06NLz+hhtuaPQYABfkgGZm6dKlTpJbt26d++ijj9z+/fvda6+95jp37uzatWvnDhw44Jxzbtq0aU6S+853vhNR//e//91JcsuWLYtYvmbNmojlhw4dcsnJyW7ixImutrY2vN3jjz/uJLlp06aFl23YsMFJchs2bHDOOffpp5+6vLw8l5ub6z755JOI/Zz/XIWFha6+/4bx6NE553Jzc11ubm6d/X3WxIkTXc+ePessr66urveYAtHgJTg0WwUFBeratau6d++uqVOnqmPHjlqxYoW6desWsd3MmTMjHi9fvlyBQEA333yzPv744/AYPHiwOnbsqA0bNkiS1q1bp1OnTumBBx6IeGls9uzZjfa2fft2lZeXa/bs2erUqVPEuvOfqyHx6vGDDz5o9OpHkk6cOCG/319n+bn3uU6cONHocwCN4SU4NFuLFy9Wnz591Lp1a2VkZOjqq69WUlLk71StW7dWTk5OxLI9e/YoGAwqPT293uc9dOiQJOm///2vJKl3794R67t27aorrrjigr2dezmwf//+F/8XauIeL6Rdu3aqqamps/zkyZPh9cClIoDQbA0dOjR8F1xD/H5/nVCqra1Venq6li1bVm9N165dY9ZjtKx7zMrK0oYNG+Sci7iyqqiokCRlZ2fHdf+4PBBAuOz06tVL69at0/Dhwy/4m3xubq6ks1cjPXv2DC//6KOP6tyJVt8+JGnXrl0qKChocLuGXo5rih4v5JprrtHLL7+sd999V5///OfDy7du3RpeD1wq3gPCZefrX/+6zpw5owULFtRZ9+mnn+rIkSOSzr7H1KZNG7344otyzoW3WbRoUaP7uPbaa5WXl6dFixaFn++c85/r3GeSPrtNvHq82NuwJ0+erDZt2ujnP/95RN8vvfSSunXrpi9+8YuNPgfQGK6AcNkZOXKkZsyYoaKiIu3YsUNjxoxRmzZttGfPHi1fvlw/+clP9NWvflVdu3bVww8/rKKiIn35y1/WhAkTtH37dv31r39Vly5dLriPpKQkLVmyRJMmTdI111yju+66S1lZWXrvvff0zjvv6G9/+5skafDgwZKkb33rWxo7dqxatWqlqVOnxq3Hi70NOycnR7Nnz9YPfvADnT59WkOGDNHKlSv197//XcuWLeNDqIgN03vwgCicuw373//+9wW3mzZtmuvQoUOD63/5y1+6wYMHu3bt2rmUlBQ3YMAA9+ijj7qDBw+Gtzlz5ox75plnXFZWlmvXrp0bNWqU27Vrl8vNzb3gbdjnbNq0yd18880uJSXFdejQwQ0cONC9+OKL4fWffvqpe+CBB1zXrl2dz+erc0t2LHt07uJvwz73vN/97nddbm6uS05Odv369XO///3vL6oWuBg+5867bgcAoInwHhAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMJFwH0Stra3VwYMHlZKSclGzBgMAEotzTkePHlV2dnaduRjPl3ABdPDgQXXv3t26DQDAJdq/f3+d2ejPl3AvwaWkpFi3AACIgcZ+nsctgBYvXqwrr7xSbdu2VX5+vv71r39dVB0vuwFAy9DYz/O4BNDrr7+uOXPmaP78+Xr77bc1aNAgjR07NvwlWgAAxGUy0qFDh7rCwsLw4zNnzrjs7GxXVFTUaG0wGHSSGAwGg9HMRzAYvODP+5hfAZ06dUrbtm2L+BKupKQkFRQUaPPmzXW2r6mpUSgUihgAgJYv5gH08ccf68yZM8rIyIhYnpGRocrKyjrbFxUVKRAIhAd3wAHA5cH8Lri5c+cqGAyGx/79+61bAgA0gZh/DqhLly5q1aqVqqqqIpZXVVUpMzOzzvZ+v19+vz/WbQAAElzMr4CSk5M1ePBgrV+/PrystrZW69ev17Bhw2K9OwBAMxWXmRDmzJmjadOm6brrrtPQoUO1aNEiVVdX66677orH7gAAzVBcAuj222/XRx99pHnz5qmyslLXXHON1qxZU+fGBADA5cvnnHPWTZwvFAopEAhYtwEAuETBYFCpqakNrje/Cw4AcHkigAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAICJ1tYNAPHQvn37qOpuvvlmzzVPPfWU55ovfOELnmv27NnjuWbBggWeayRp2bJlUdUBXnAFBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwITPOeesmzhfKBRSIBCwbgMJZOTIkZ5rvvvd70a1r/z8fM81Pp/Pc02C/bero3Vr5inGpQsGg0pNTW1wPVdAAAATBBAAwETMA+jpp5+Wz+eLGH379o31bgAAzVxcXujt16+f1q1b93874fVkAMBnxCUZWrdurczMzHg8NQCghYjLe0B79uxRdna2evbsqTvvvFP79u1rcNuamhqFQqGIAQBo+WIeQPn5+SouLtaaNWu0ZMkSlZeX60tf+pKOHj1a7/ZFRUUKBALh0b1791i3BABIQHH/HNCRI0eUm5urF154QXfffXed9TU1NaqpqQk/DoVChBAi8Dmgpsf7toiFxj4HFPezrFOnTurTp4/27t1b73q/3y+/3x/vNgAACSbunwM6duyYysrKlJWVFe9dAQCakZgH0MMPP6zS0lJ98MEH+uc//6lbbrlFrVq10h133BHrXQEAmrGYvwR34MAB3XHHHTp8+LC6du2qG264QVu2bFHXrl1jvSsAQDPGZKRoUi+++KLnmttvv91zTVpamueaaP3xj3/0XHP99dd7rsnJyfFcE60pU6Z4rlm9enXsG0GzxmSkAICERAABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwARfewj16dMnqroVK1Z4runbt6/nmqacL/dXv/qV55qZM2d6rnn99dc91zTlZKR/+tOfPNdUVFR4rlmwYIHnmpdfftlzDRITV0AAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABPMhg0NHz48qrpoZrZOSvL+O09tba3nmlmzZnmukaQlS5ZEVefVunXrPNd85Stf8VyTnJzsuUaSWrf2/qMhmtm6f/GLX3iumTFjhueam266yXONJB09ejSqOlwcroAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCY8DnnnHUT5wuFQgoEAtZtNFt9+vTxXLNp06ao9pWWlua5xufzea6JZkLIaCdYfeedd6Kqawo33nij55rHHnssqn2lpKR4rundu7fnmmjOoWj885//jKrulltu8Vxz+PDhqPbVEgWDQaWmpja4nisgAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJpiMtIVZtGiR55pZs2bFvpEGRDMp5MyZMz3XJPKkoi1VNBPhPv/8855ropkgNJpJcCXp/fff91zzta99zXPNrl27PNc0B0xGCgBISAQQAMCE5wDauHGjJk2apOzsbPl8Pq1cuTJivXNO8+bNU1ZWltq1a6eCggLt2bMnVv0CAFoIzwFUXV2tQYMGafHixfWuX7hwoX7605/qpZde0tatW9WhQweNHTtWJ0+evORmAQAtR2uvBePHj9f48ePrXeec06JFi/Tkk09q8uTJkqTf/va3ysjI0MqVKzV16tRL6xYA0GLE9D2g8vJyVVZWqqCgILwsEAgoPz9fmzdvrrempqZGoVAoYgAAWr6YBlBlZaUkKSMjI2J5RkZGeN1nFRUVKRAIhEf37t1j2RIAIEGZ3wU3d+5cBYPB8Ni/f791SwCAJhDTAMrMzJQkVVVVRSyvqqoKr/ssv9+v1NTUiAEAaPliGkB5eXnKzMzU+vXrw8tCoZC2bt2qYcOGxXJXAIBmzvNdcMeOHdPevXvDj8vLy7Vjxw6lpaWpR48emj17tp577jn17t1beXl5euqpp5Sdna0pU6bEsm8AQDPnOYDeeust3XjjjeHHc+bMkSRNmzZNxcXFevTRR1VdXa17771XR44c0Q033KA1a9aobdu2sesaANDsMRlpAktJSfFcU1pa6rlm4MCBnmuidcUVV3iuOXr0aBw6QSJo376955rzP+ZxsVasWOG5Rjr72UaviouLPdd885vf9FzTHDAZKQAgIRFAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATDAbdgLr3r2755oPPvgg9o004NixY55r+LeFhWeeeSaquieffDLGndSvX79+nmvee++9OHQSW8yGDQBISAQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEy0tm4ADZs4caLnmqacW/aXv/xlk+0LuBQvv/xyVHVPPPFEjDup39133+255pFHHolDJ02LKyAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmmIw0gfXu3du6BaBFOH78eFR1Bw4c8FyTk5MT1b4uR1wBAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMMFkpABaPL/fH1Vd586dY9wJzscVEADABAEEADDhOYA2btyoSZMmKTs7Wz6fTytXroxYP336dPl8vogxbty4WPULAGghPAdQdXW1Bg0apMWLFze4zbhx41RRUREer7766iU1CQBoeTzfhDB+/HiNHz/+gtv4/X5lZmZG3RQAoOWLy3tAJSUlSk9P19VXX62ZM2fq8OHDDW5bU1OjUCgUMQAALV/MA2jcuHH67W9/q/Xr1+v73/++SktLNX78eJ05c6be7YuKihQIBMKje/fusW4JAJCAYv45oKlTp4b/PGDAAA0cOFC9evVSSUmJRo8eXWf7uXPnas6cOeHHoVCIEAKAy0Dcb8Pu2bOnunTpor1799a73u/3KzU1NWIAAFq+uAfQgQMHdPjwYWVlZcV7VwCAZsTzS3DHjh2LuJopLy/Xjh07lJaWprS0ND3zzDO67bbblJmZqbKyMj366KO66qqrNHbs2Jg2DgBo3jwH0FtvvaUbb7wx/Pjc+zfTpk3TkiVLtHPnTv3mN7/RkSNHlJ2drTFjxmjBggVRz8UEAGiZPAfQqFGj5JxrcP3f/va3S2oI/+fYsWOea3w+Xxw6qd+oUaOabF/Apbjpppuiqmvfvr3nmuPHj3uueeGFFzzXtATMBQcAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMOFzF5ra2kAoFFIgELBuIyFE89Xk5eXlcegkdlq3jvm3wAONeu+996Kqu+qqqzzX/PGPf/RcM3XqVM81zUEwGLzgt1xzBQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEM0MmsOPHj3uuOXDggOeanJwczzXRmjdvnueaZ599Ng6doLmK5hzq3bt3VPuKZq7m999/P6p9XY64AgIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCyUgT2OHDhz3XLFmyxHPN888/77kmWk8++aTnmmgmhFywYIHnGjS9aCYWfeKJJ+LQSf3+8pe/eK759a9/HYdOWiaugAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJjwuWhmeoyjUCikQCBg3cZlJZoJIaXoJhZt3dr7/LfRnKJ79uzxXCNJU6ZM8VwTCoU819TU1Hiuad++veeaaI0cOdJzTTTnQ+/evT3XROPDDz+Mqq6goMBzzfvvvx/VvlqiYDCo1NTUBtdzBQQAMEEAAQBMeAqgoqIiDRkyRCkpKUpPT9eUKVO0e/fuiG1OnjypwsJCde7cWR07dtRtt92mqqqqmDYNAGj+PAVQaWmpCgsLtWXLFq1du1anT5/WmDFjVF1dHd7moYce0p///GctX75cpaWlOnjwoG699daYNw4AaN48vSO8Zs2aiMfFxcVKT0/Xtm3bNGLECAWDQf3617/WK6+8optuukmStHTpUn3uc5/Tli1bdP3118eucwBAs3ZJ7wEFg0FJUlpamiRp27ZtOn36dMSdI3379lWPHj20efPmep+jpqZGoVAoYgAAWr6oA6i2tlazZ8/W8OHD1b9/f0lSZWWlkpOT1alTp4htMzIyVFlZWe/zFBUVKRAIhEf37t2jbQkA0IxEHUCFhYXatWuXXnvttUtqYO7cuQoGg+Gxf//+S3o+AEDz4P1TgZJmzZql1atXa+PGjcrJyQkvz8zM1KlTp3TkyJGIq6CqqiplZmbW+1x+v19+vz+aNgAAzZinKyDnnGbNmqUVK1bozTffVF5eXsT6wYMHq02bNlq/fn142e7du7Vv3z4NGzYsNh0DAFoET1dAhYWFeuWVV7Rq1SqlpKSE39cJBAJq166dAoGA7r77bs2ZM0dpaWlKTU3VAw88oGHDhnEHHAAggqcAWrJkiSRp1KhREcuXLl2q6dOnS5J+/OMfKykpSbfddptqamo0duxY/fznP49JswCAloPJSBG1p556ynPN008/7bkmwU7ROk6cOOG55n//+5/nmm7dunmuiZbP5/Nc01T/ThUVFZ5rRo8eHdW+mFj00jAZKQAgIRFAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATDAbNppUNF/hHs13SZ3/Tb3xlsgzR0erqf5OBw4c8FwzceJEzzXvvPOO5xpcOmbDBgAkJAIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACaYjBQJr3Pnzp5r7rrrrqj2lZGR4blmzpw5nmui+W/3/vvve66prKz0XCNJb7/9tueav/zlL55rdu7c6bnm8OHDnmtgg8lIAQAJiQACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkmIwUAxAWTkQIAEhIBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEx4CqCioiINGTJEKSkpSk9P15QpU7R79+6IbUaNGiWfzxcx7rvvvpg2DQBo/jwFUGlpqQoLC7VlyxatXbtWp0+f1pgxY1RdXR2x3T333KOKiorwWLhwYUybBgA0f629bLxmzZqIx8XFxUpPT9e2bds0YsSI8PL27dsrMzMzNh0CAFqkS3oPKBgMSpLS0tIili9btkxdunRR//79NXfuXB0/frzB56ipqVEoFIoYAIDLgIvSmTNn3MSJE93w4cMjlv/iF79wa9ascTt37nS///3vXbdu3dwtt9zS4PPMnz/fSWIwGAxGCxvBYPCCORJ1AN13330uNzfX7d+//4LbrV+/3klye/furXf9yZMnXTAYDI/9+/ebHzQGg8FgXPpoLIA8vQd0zqxZs7R69Wpt3LhROTk5F9w2Pz9fkrR371716tWrznq/3y+/3x9NGwCAZsxTADnn9MADD2jFihUqKSlRXl5eozU7duyQJGVlZUXVIACgZfIUQIWFhXrllVe0atUqpaSkqLKyUpIUCATUrl07lZWV6ZVXXtGECRPUuXNn7dy5Uw899JBGjBihgQMHxuUvAABopry876MGXudbunSpc865ffv2uREjRri0tDTn9/vdVVdd5R555JFGXwc8XzAYNH/dksFgMBiXPhr72e/7/8GSMEKhkAKBgHUbAIBLFAwGlZqa2uB65oIDAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJhIuAByzlm3AACIgcZ+nidcAB09etS6BQBADDT289znEuySo7a2VgcPHlRKSop8Pl/EulAopO7du2v//v1KTU016tAex+EsjsNZHIezOA5nJcJxcM7p6NGjys7OVlJSw9c5rZuwp4uSlJSknJycC26Tmpp6WZ9g53AczuI4nMVxOIvjcJb1cQgEAo1uk3AvwQEALg8EEADARLMKIL/fr/nz58vv91u3YorjcBbH4SyOw1kch7Oa03FIuJsQAACXh2Z1BQQAaDkIIACACQIIAGCCAAIAmCCAAAAmmk0ALV68WFdeeaXatm2r/Px8/etf/7Juqck9/fTT8vl8EaNv377WbcXdxo0bNWnSJGVnZ8vn82nlypUR651zmjdvnrKystSuXTsVFBRoz549Ns3GUWPHYfr06XXOj3Hjxtk0GydFRUUaMmSIUlJSlJ6erilTpmj37t0R25w8eVKFhYXq3LmzOnbsqNtuu01VVVVGHcfHxRyHUaNG1Tkf7rvvPqOO69csAuj111/XnDlzNH/+fL399tsaNGiQxo4dq0OHDlm31uT69eunioqK8Ni0aZN1S3FXXV2tQYMGafHixfWuX7hwoX7605/qpZde0tatW9WhQweNHTtWJ0+ebOJO46ux4yBJ48aNizg/Xn311SbsMP5KS0tVWFioLVu2aO3atTp9+rTGjBmj6urq8DYPPfSQ/vznP2v58uUqLS3VwYMHdeuttxp2HXsXcxwk6Z577ok4HxYuXGjUcQNcMzB06FBXWFgYfnzmzBmXnZ3tioqKDLtqevPnz3eDBg2ybsOUJLdixYrw49raWpeZmel+8IMfhJcdOXLE+f1+9+qrrxp02DQ+exycc27atGlu8uTJJv1YOXTokJPkSktLnXNn/+3btGnjli9fHt7m3XffdZLc5s2brdqMu88eB+ecGzlypHvwwQftmroICX8FdOrUKW3btk0FBQXhZUlJSSooKNDmzZsNO7OxZ88eZWdnq2fPnrrzzju1b98+65ZMlZeXq7KyMuL8CAQCys/PvyzPj5KSEqWnp+vqq6/WzJkzdfjwYeuW4ioYDEqS0tLSJEnbtm3T6dOnI86Hvn37qkePHi36fPjscThn2bJl6tKli/r376+5c+fq+PHjFu01KOFmw/6sjz/+WGfOnFFGRkbE8oyMDL333ntGXdnIz89XcXGxrr76alVUVOiZZ57Rl770Je3atUspKSnW7ZmorKyUpHrPj3PrLhfjxo3Trbfeqry8PJWVlenxxx/X+PHjtXnzZrVq1cq6vZirra3V7NmzNXz4cPXv31/S2fMhOTlZnTp1iti2JZ8P9R0HSfrGN76h3NxcZWdna+fOnXrssce0e/du/elPfzLsNlLCBxD+z/jx48N/HjhwoPLz85Wbm6s//OEPuvvuuw07QyKYOnVq+M8DBgzQwIED1atXL5WUlGj06NGGncVHYWGhdu3adVm8D3ohDR2He++9N/znAQMGKCsrS6NHj1ZZWZl69erV1G3WK+FfguvSpYtatWpV5y6WqqoqZWZmGnWVGDp16qQ+ffpo79691q2YOXcOcH7U1bNnT3Xp0qVFnh+zZs3S6tWrtWHDhojvD8vMzNSpU6d05MiRiO1b6vnQ0HGoT35+viQl1PmQ8AGUnJyswYMHa/369eFltbW1Wr9+vYYNG2bYmb1jx46prKxMWVlZ1q2YycvLU2ZmZsT5EQqFtHXr1sv+/Dhw4IAOHz7cos4P55xmzZqlFStW6M0331ReXl7E+sGDB6tNmzYR58Pu3bu1b9++FnU+NHYc6rNjxw5JSqzzwfouiIvx2muvOb/f74qLi91//vMfd++997pOnTq5yspK69aa1Le//W1XUlLiysvL3T/+8Q9XUFDgunTp4g4dOmTdWlwdPXrUbd++3W3fvt1Jci+88ILbvn27++9//+ucc+573/ue69Spk1u1apXbuXOnmzx5ssvLy3MnTpww7jy2LnQcjh496h5++GG3efNmV15e7tatW+euvfZa17t3b3fy5Enr1mNm5syZLhAIuJKSEldRUREex48fD29z3333uR49erg333zTvfXWW27YsGFu2LBhhl3HXmPHYe/eve7ZZ591b731lisvL3erVq1yPXv2dCNGjDDuPFKzCCDnnHvxxRddjx49XHJyshs6dKjbsmWLdUtN7vbbb3dZWVkuOTnZdevWzd1+++1u79691m3F3YYNG5ykOmPatGnOubO3Yj/11FMuIyPD+f1+N3r0aLd7927bpuPgQsfh+PHjbsyYMa5r166uTZs2Ljc3191zzz0t7pe0+v7+ktzSpUvD25w4ccLdf//97oorrnDt27d3t9xyi6uoqLBrOg4aOw779u1zI0aMcGlpac7v97urrrrKPfLIIy4YDNo2/hl8HxAAwETCvwcEAGiZCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGDi/wFx1YbVoboPwwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "# Flatten the images from 28x28 to 784\n",
        "x_train = x_train.reshape(x_train.shape[0], 28 * 28).astype('float32') / 255.0\n",
        "x_test = x_test.reshape(x_test.shape[0], 28 * 28).astype('float32') / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build the MLP model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, activation='relu', input_shape=(28 * 28,)))\n",
        "model.add(Dense(64, activation='relu'))  # Add more layers if needed\n",
        "model.add(Dense(10, activation='softmax'))  # Output layer for 10 classes (digits 0-9)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=128, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Accuracy: {test_acc}\")\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(x_test)\n",
        "\n",
        "# Optionally visualize predictions on a sample test image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Choose a random image from test set\n",
        "random_index = np.random.randint(0, x_test.shape[0])\n",
        "plt.imshow(x_test[random_index].reshape(28, 28), cmap='gray')\n",
        "plt.title(f\"Predicted: {np.argmax(predictions[random_index])}\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}